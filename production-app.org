* Production (like) App

This is an example of what a production Jackdaw app would roughly look like
using Literate programming.

** Project Setup

The initial setup is fairly standard. Should not be too different from your
regular Clojure app setups. We'll include resources and bin folder for
supporting files for deployment. We'll also include a sample Circle CI
configuration mostly for illustration purposes.

#+BEGIN_SRC bash :results silent
mkdir -p prod-app/resources
mkdir -p prod-app/resources/schemas
mkdir -p prod-app/src/prod_app
mkdir -p prod-app/src/prod_app/specs
mkdir -p prod-app/test/prod_app
mkdir -p prod-app/test/resources
mkdir -p prod-app/test/prod_app/integration
mkdir -p prod-app/dev
mkdir -p prod-app/bin
mkdir -p prod-app/.circleci
#+END_SRC

** Dependencies

  Lets start with our deps file. We will include everything needed for building a
  production ready Jackdaw app. I'll go over the highlights after the snippet.

#+begin_src clojure :tangle prod-app/deps.edn :eval no
{:paths ["src" "resources"]

 :deps {org.clojure/clojure {:mvn/version "1.10.1"}
        ;; Core App
        fundingcircle/jackdaw {:mvn/version "0.7.4"
                               :exclusions [org.apache.zookeeper/zookeeper
                                            org.slf4j/slf4j-log4j12]}
        integrant {:mvn/version "0.7.0"}
        com.outpace/config {:mvn/version "0.13.2"}

        ;; Support
        danlentz/clj-uuid {:mvn/version "0.1.9"}
        org.clojure/algo.generic {:mvn/version "0.1.3"}
        org.clojure/data.json {:mvn/version "1.0.0"}
        org.clojure/data.zip {:mvn/version "1.0.0"}
        org.clojure/tools.logging {:mvn/version "1.0.0"}
        clj-http {:mvn/version "3.10.1"}

        ;; Logging
        ch.qos.logback/logback-classic {:mvn/version "1.2.3"}
        cambium/cambium.core {:mvn/version "0.9.3"}
        cambium/cambium.codec-cheshire {:mvn/version "0.9.3"}
        cambium/cambium.logback.json {:mvn/version "0.4.3"
                                      :exlusions [ch.qos.logback/logback-classic]}
        cambium/cambium.logback.core {:mvn/version "0.4.3"
                                      :exclusions [org.slf4j/slf4j-log4j12]}
        ;; Metrics and reporting
        io.prometheus.jmx/jmx_prometheus_javaagent {:mvn/version "0.12.0"}
        camdez/honeybadger {:mvn/version "0.4.1"}
        metrics-clojure {:mvn/version "2.10.0"}}

 :aliases
 {:dev
  {:extra-paths ["dev" "test" "test/resources"]
   :extra-deps {integrant/repl {:mvn/version "0.3.1"}
                org.apache.kafka/kafka-streams-test-utils {:mvn/version "2.3.1"}
                org.apache.kafka/kafka_2.11 {:mvn/version "2.3.1"
                                             :exclusions [org.slf4j/slf4j-log4j12]}}}

  :check {:extra-deps {athos/clj-check
                       {:git/url "https://github.com/athos/clj-check.git"
                        :sha "b48d4e7000586529f81c1e29069b503b57259514"}}
          :main-opts ["-m" "clj-check.check"]}

  :test {:extra-paths ["test" "test/resources"]
         :extra-deps {com.cognitect/test-runner
                      {:git/url "https://github.com/cognitect-labs/test-runner.git"
                       :sha "209b64504cb3bd3b99ecfec7937b358a879f55c1"}
                      org.clojure/test.check {:mvn/version "0.9.0"}
                      org.apache.kafka/kafka-streams-test-utils {:mvn/version "2.3.1"}
                      org.apache.kafka/kafka_2.11 {:mvn/version "2.3.1"
                                                   :exclusions [org.slf4j/slf4j-log4j12]}}
         :main-opts ["-m" "cognitect.test-runner"]}

  :uberjar {:extra-deps {seancorfield/depstar {:mvn/version "0.5.2"}}
            :main-opts ["-m" "hf.depstar.uberjar" "sba-connector.jar"]}}

 :mvn/local-repo ".m2"

 :mvn/repos
 {"confluent" {:url "https://packages.confluent.io/maven/"}}}
#+end_src

  Now that we have a deps file if you want to run the rest of the clojure
  snippets be sure to connect to a REPL with these deps. I recommend jumping to
  the tangled ~deps.edn~ and running ~cider-jack-in-clj~ and then jumping back
  to this file and running ~cider-connect-clj~ with the host and port used by
  your previous jack-in command.

*** Core app.
  
  The main recommended components of a Jackdaw streams app. We recommend
  [[https://github.com/weavejester/integrant][Integrant]] to assemble and run Jackdaw applications. We'll see how that is done
  later on. Loading external configuration on you system has been something that
  tends to vary a lot in our apps. We've recently been giving [[https://github.com/outpace/config][Outpace config]] a
  try. I personally like the approach this library takes and its features but
  you could as easily use any other configuration library out there with minor
  modifications.

*** Logging

  For logging I have found that the [[https://cambium-clojure.github.io/][Cambium]] library offers the best features and
  flexibility for most structured logging setups. Setting up logging
  successfully in any Clojure app can be a pain in the butt. Cambium is
  implemented on top of Logback which I consider the best in class logging
  solution for JVM based setups. It works well with the underlying Kafka
  libraries as well as our own logging statements. Other solutions fall short in
  one way or other. This one is a great middle-ground between simplicity and
  power.

*** Metrics and reporting

  This section is totally dependent on your personal infrastructure. We use
  prometheus for real time metrics and dashboards and honeybadger for exception
  reporting. I will provide setup for an environment with such tooling for demo
  purposes but keep in mind this can vary greatly.

*** Support and Misc.

  There are other libraries that we use for support for example depstar for
  building our jarfile, test-runner to run our tests locally and on CI, etc.
  This support dependencies will become self explaining as we walk thought this
  demo. One last note, we set ~:mvn/local-repo~ for our CI setup so that we can
  easily cache dependencies between builds.
  
** Overview
 
  Onto the main course. For this example we'll showcase a simple topology
  similar to the one we implemented to submit loans to the SBA. This example
  omits a ton of details around authenticating and communicating with external
  services nor any details about how we actually deal with loans or customer
  information. It is meant for illustrative purposes only.

*** The problem.

  A global pandemic is underway and small businesses are badly hit due go
  lock-down measures. The government put in place a program to help small
  businesses stay open and help them keep employees on payroll. The government
  creates the Payment Protection Program (PPP) and Funding Circle is approved
  as a lender. The goal now is to build an app as quickly as possible to submit
  loans to the Small Business Administration (SBA) for approval before the
  funding of the program starts.

  Fortunately we have already systems in place to originate and service loans.
  We have teams already working on the marketing and sales part that would
  gather all the info from customers wishing to apply for a PPP loan. The goal
  is to create a service that will gather and prepare loan data for submission
  to the SBA in a format that they support. The SBA requires loans to have
  specific information in a specific format. We also want to avoid sending
  incomplete or invalid data as there are request limits per lender. We have to
  collect data in real time and inform upstream systems (Salesforce in this
  case) when there is enough valid data for a loan to be submitted. Finally,
  when an underwriter is ready to submit a loan they can trigger an event that
  flows down to our system.

  Let's see generally how that looks:

#+BEGIN_SRC dot :file prod-app/topology.png :cmdline -Kdot -Tpng :exports results
digraph Topology {
  bgcolor="white";
  external_loan_application [shape=box];
  external_loan_application -> update_sba_loan;
  sba_loan_application_updated [shape=box];
  update_sba_loan -> sba_loan_application_updated;
  external_loan_submit_triggered [shape=box];
  external_loan_submit_triggered -> send_loan_application_to_sba;
  sba_results_available [shape=box];
  send_loan_application_to_sba -> sba_results_available;
  state_store [shape=cylinder];
  state_store -> update_sba_loan;
  update_sba_loan -> state_store;
  state_store -> send_loan_application_to_sba; 
}
#+END_SRC

#+RESULTS:
[[file:prod-app/topology.png]]

  Now that we have a general understanding of what our app should do lets
  go ahead and implement it.

** Supporting code

  Jackdaw allows you to create a Kafka Streams app with very little code. There
  are some great [[https://github.com/FundingCircle/jackdaw/tree/master/examples][examples]] in the Jackdaw repository. I suggest you also look at
  those. For production ready apps there is a bit more of scaffolding or support
  code that we need.

*** Data model

  A great place to start is always the data model. In Clojure we use specs for
  that. Lets start with the base attributes on our data model. NOTE: This is a
  simplified data model.

#+BEGIN_SRC clojure :tangle prod-app/src/prod_app/specs/attributes.clj :results silent
(ns prod-app.specs.attributes
  "This namespace contains attribute specs."
  (:require [clojure.string :as str]
            [clojure.spec.alpha :as s]
            [clojure.spec.gen.alpha :as gen] ))

(s/def :external/opportunity-id string?)
(s/def :external/trigger-id uuid?)

(s/def :loan-application/requested-amount string?)
(s/def :loan-application/loan-application-id uuid?)
(s/def :loan-application/tax-id 
  (s/with-gen #(re-matches #"[0-9]{10}" %)
    #(gen/return (str/join (map str (take 10 (repeatedly (fn [] (rand-int 10)))))))))
(s/def :loan-application/loan-application-is-complete boolean?)
(s/def :loan-application/problem string?)
(s/def :loan-application/problems
  (s/* :loan-application/problem))

(s/def :metadata/id uuid?)
(s/def :metadata/published-timestamp int?)
(s/def :metadata/published-by string?)

(s/def :sba-response/status #{"success" "failure" "cancelled"})
(s/def :sba-response/result string?)
(s/def :sba-response/loan-number (s/nilable string?))
#+END_SRC

  Now we define our reader specs. For this demo the only difference between an
  external and internal loan application is only ~loan-application-id~ being
  absent in the external one.

#+BEGIN_SRC clojure :tangle prod-app/src/prod_app/specs/reader_specs.clj :results silent
(ns prod-app.specs.reader-specs
  "Spec for reads from internal state and message streams.
  Use this spec for validation AFTER READING FROM INTERNAL STATE or
  reading messages from Kafka"
  (:require [clojure.spec.alpha :as s]
            [prod-app.specs.attributes]))

(s/def ::loan-application
  (s/keys :req-un [:external/opportunity-id
                   :loan-application/requested-amount
                   :loan-application/loan-application-id
                   :loan-application/tax-id]))

(s/def ::external-loan-application
  (s/keys :req-un [:external/opportunity-id
                   :loan-application/requested-amount
                   :loan-application/tax-id]))

(s/def ::external-trigger
  (s/keys :req-un [:external/opportunity-id
                   :external/trigger-id]))
#+END_SRC

  A loan application with invalid tax id.

#+begin_src clojure :tangle prod-app/dev/scratch.clj :exports both
(s/explain-data ::loan-application 
                {:loan-application-id (java.util.UUID/randomUUID)
                 :opportunity-id "external-id-for-a-loan"
                 :requested-amount "100"
                 :tax-id "foo"})
#+end_src

#+RESULTS:
: #:clojure.spec.alpha{:problems ({:path [:tax-id], :pred :clojure.spec.alpha/unknown, :val "foo", :via [:prod-app.specs.reader-specs/loan-application :loan-application/tax-id], :in [:tax-id]}), :spec :prod-app.specs.reader-specs/loan-application, :value {:loan-application-id #uuid "503d7ae5-aa4c-4074-a854-50493c7ee317", :opportunity-id "external-id-for-a-loan", :requested-amount "100", :tax-id "foo"}}

  A valid loan application entry in our state store.

#+begin_src clojure :tangle prod-app/dev/scratch.clj :results value :exports both
(s/valid? ::loan-application
          {:loan-application-id (java.util.UUID/randomUUID)
           :opportunity-id "external-id-for-a-loan"
           :requested-amount "100"
           :tax-id "1111111111"})
#+end_src

#+RESULTS:
: true

  This is what an ~external-trigger~ event looks like.

#+begin_src clojure :tangle prod-app/dev/scratch.clj :results value :exports both
(s/valid? ::external-trigger
          {:trigger-id (java.util.UUID/randomUUID)
           :opportunity-id "external-id-for-a-loan"})
#+end_src

#+RESULTS:
: true

  Writer specs are the specs we use to write to Kafka topics. This specs are
  usually less stringent as they only require the minimum data required for us
  consider a valid topic. For example notice how both reader and writer specs
  define ~::loan-application~. The reason for the difference is that when we write
  to the topic we are just aggregating data coming from the upstream external
  topics. The reader specs validate that a loan application is considered
  complete. Our topology will produce to ~sba-loan-updated-event~ with the right
  value for ~:loan-application/loan-application-is-complete~.

#+BEGIN_SRC clojure :tangle prod-app/src/prod_app/specs/writer_specs.clj :results silent
(ns prod-app.specs.writer-specs
  "Spec for writes to internal state and message streams.
  Use this spec for validation BEFORE WRITING TO INTERNAL STATE or publishing messages to Kafka."
  (:require [clojure.spec.alpha :as s]
            [prod-app.specs.attributes]))

(s/def ::loan-application
  (s/keys :req-un [:metadata/loan-application-id]
          :opt-un [:loan-application/requested-amount
                   :external/opportunity-id
                   :loan-application/tax-id]))

(s/def ::result
  (s/keys :req-un [:sba-response/status
                   :sba-response/result]
          :opt-un [:sba-response/loan-number]))

(s/def ::metadata
  (s/keys :req-un [:metadata/id
                   :metadata/published-timestamp
                   :metadata/published-by]))

(s/def ::sba-loan-application-updated-event
  (s/merge ::loan-application
           (s/keys :req-un [:loan-application/loan-application-is-complete])
           ::metadata
           (s/keys :req-un [:loan-application/problems])))

(s/def ::sba-result-available-event
  (s/merge ::result
           ::loan-application
           ::metadata))
#+END_SRC

  This is an example of an entry we expect on the ~sba-loan-application-updated-event~

#+begin_src clojure :tangle prod-app/dev/scratch.clj :results value :exports both
(s/valid? ::sba-loan-application-updated-event
          {:loan-application-id (java.util.UUID/randomUUID)
           :loan-application-is-complete true
           :problems []
           :opportunity-id "external-id-for-a-loan"
           :requested-amount "100"
           :tax-id "1111111111"
           :id (java.util.UUID/randomUUID)
           :published-timestamp 1
           :published-by "test"})
#+end_src

#+RESULTS:
: true

  This is an example of an entry we expect on the ~sba-result-available-event~

#+begin_src clojure :tangle prod-app/dev/scratch.clj :results value :exports both
(s/valid? ::sba-result-available-event
          {:loan-application-id (java.util.UUID/randomUUID)
           :opportunity-id "external-id-for-a-loan"
           :requested-amount "100"
           :tax-id "1111111111"
           :id (java.util.UUID/randomUUID)
           :published-timestamp 1
           :published-by "test"
           :status "success"
           :result ""
           :loan-number "123"})
#+end_src

#+RESULTS:
: true

*** Logging and metrics

We'll define a logging namespace that can be used by other namespaces in our
application. Instead of directly calling the logging libraries API we mask them
with our own. This has the benefit of being able to switch logging back-ends
more easily and decorate log entries as we see fit. In this case we will are
able to create a custom logging function that can also produce metrics for
specific logging events. This becomes a super powerful way to be able to
diagnose and track the health of our application.

#+BEGIN_SRC clojure :tangle prod-app/src/prod_app/log.clj :results silent
(ns prod-app.log
  "Thin wrappers around cambium's logging fns."
  (:require [cambium.codec :as codec]
            [cambium.core :as cambium-core]
            [cambium.logback.json.flat-layout :as flat]
            [clojure.set :as set]
            [metrics.meters :as meters]))

;; See https://cambium-clojure.github.io/documentation.html#cambium-codec
(flat/set-decoder! codec/destringify-val)

(defmacro debug
  "structured log at the INFO level"
  {:arglists '([msg] [mdc msg] [mdc throwable msg])}
  [& args]
  `(cambium-core/debug ~@args))

(defmacro info
  "structured log at the INFO level"
  {:arglists '([msg] [mdc msg] [mdc throwable msg])}
  [& args]
  `(cambium-core/info ~@args))

(defmacro warn
  "structured log at the WARN level"
  {:arglists '([msg] [mdc msg] [mdc throwable msg])}
  [& args]
  `(cambium-core/warn ~@args))

(defmacro error
  "structured log at the ERROR level"
  {:arglists '([msg] [mdc msg] [mdc throwable msg])}
  [& args]
  `(cambium-core/error ~@args))

(defn ->metric-name [title]
  ["sba-connector" "event" title])

(defn test-metrics [metrics-registry]
  (meters/mark! (meters/meter metrics-registry (->metric-name "test-event"))))

(defn logger
  "Super logger function"
  [{:keys [level event message throwable metrics-registry]
          :or {level :info
               message ""
               event "unknown-event"
               throwable nil
               metrics-registry nil}
          :as all-keys}
   & things]
  (let [other-keys (apply (partial dissoc all-keys) [:level :event :message :metrics-registry])
        log-fn #(cambium-core/log level % throwable message)]
    (as-> (apply merge things) mdc
      (select-keys mdc [:loan-application-id :loan-application-is-complete :problems
                        :opportunity-id :requested-amount :tax-id :id :published-timestamp
                        :published-by])
      (merge mdc
             {:event event}
             other-keys)
      (log-fn mdc)))
  ;; Record event in metrics
  (when metrics-registry
    (meters/mark! (meters/meter metrics-registry (->metric-name event)))))
#+END_SRC

We also need to define our logback configurations for test and production.

#+BEGIN_SRC xml :tangle prod-app/test/resources/logback-test.xml :eval no 
<configuration>
    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <encoder class="ch.qos.logback.core.encoder.LayoutWrappingEncoder">
            <layout class="cambium.logback.json.FlatJsonLayout">
                <jsonFormatter class="ch.qos.logback.contrib.jackson.JacksonJsonFormatter">
                    <prettyPrint>true</prettyPrint>
                </jsonFormatter>
                <timestampFormat>yyyy-MM-dd'T'HH:mm:ss.SSS'Z'</timestampFormat>
                <timestampFormatTimezoneId>UTC</timestampFormatTimezoneId>
                <appendLineSeparator>true</appendLineSeparator>
            </layout>
        </encoder>
        <file>log/sba-connector-test.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
            <!-- rollover daily -->
            <fileNamePattern>log/sba-connector-%d{yyyy-MM-dd}.%i.log</fileNamePattern>

            <!-- each file should be at most 64MB, keep 14 days worth of history, but at most 512MB -->
            <maxFileSize>64MB</maxFileSize>
            <maxHistory>14</maxHistory>
            <totalSizeCap>512MB</totalSizeCap>

            <!-- short-lived processes should clean up old logs -->
            <cleanHistoryOnStart>true</cleanHistoryOnStart>
        </rollingPolicy>
    </appender>

    <root level="INFO">
        <appender-ref ref="FILE" />
    </root>
</configuration>
#+END_SRC

#+BEGIN_SRC xml :tangle prod-app/resources/logback.xml :eval no 
<configuration>
    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="ch.qos.logback.core.encoder.LayoutWrappingEncoder">
            <layout class="cambium.logback.json.FlatJsonLayout">
                <jsonFormatter class="ch.qos.logback.contrib.jackson.JacksonJsonFormatter">
                </jsonFormatter>
                <timestampFormat>yyyy-MM-dd'T'HH:mm:ss.SSS'Z'</timestampFormat>
                <timestampFormatTimezoneId>UTC</timestampFormatTimezoneId>
                <appendLineSeparator>true</appendLineSeparator>
            </layout>
        </encoder>
    </appender>

    <turboFilter class="cambium.logback.core.StrategyTurboFilter">
        <name>nsStrategy</name>
    </turboFilter>

    <root level="INFO">
        <appender-ref ref="STDOUT" />
    </root>
</configuration>
#+END_SRC

*** Transducers

The Kafka Streams DSL models streams apps as Topologies where transformations
are applied to collections of data (topics). It provides abstractions like map,
filter, flatmap, etc. This abstractions are all too common for Clojure
developers. Jackdaw makes those transformations look like regular Clojure code.

However the Kafka Streams DSL does not support composable transformations like
Clojure can via transducers. Having said that, there is no reason we cant take
advantage of the amazing properties of transducers in our Jackdaw applications.
In my opinion the main benefit is being able to rely only on unit tests for all
of the business logic related to a topology. We will still have integration
tests but we will rely much less on them when we use transducers.

Jackdaw does not have support for transducers yet. This is a prototype
implementation we will use for this project.

#+BEGIN_SRC clojure :tangle prod-app/src/prod_app/xform.clj :results silent
(ns prod-app.xform
  "Helper functions for working with transducers."
  (:gen-class)
  (:refer-clojure :exclude [transduce])
  (:require [jackdaw.serdes :as js]
            [jackdaw.streams :as j])
  (:import org.apache.kafka.streams.kstream.Transformer
           [org.apache.kafka.streams.state KeyValueStore Stores]
           org.apache.kafka.streams.StreamsBuilder))


(defn fake-kv-store
  "Creates an instance of org.apache.kafka.streams.state.KeyValueStore
  with overrides for get and put."
  [init]
  (let [store (volatile! init)]
    (reify KeyValueStore
      (get [_ k]
        (clojure.core/get @store k))

      (put [_ k v]
        (vswap! store assoc k v)))))


(defn kv-store-get-fn
  "Takes an instance of KeyValueStore and a key k, and gets a value
  from the store in a manner similar to `clojure.core/get`."
  [^KeyValueStore store k]
  (.get store k))


(defn kv-store-swap-fn
  "Takes an instance of KeyValueStore, a function f, and map m, and
  updates the store in a manner similar to `clojure.core/swap!`."
  [^KeyValueStore store f m]
  (let [ks (keys (f {} m))
        prev (reduce (fn [p k]
                       (assoc p k (.get store k)))
                     {}
                     ks)
        next (f prev m)]
    (doall (map (fn [[k v]] (.put store k v)) next))
    next))


(defn add-state-store!
  "Takes a builder and adds a state store."
  [builder]
  (doto ^StreamsBuilder (j/streams-builder* builder)
    (.addStateStore (Stores/keyValueStoreBuilder
                     (Stores/persistentKeyValueStore "state")
                     (js/edn-serde)
                     (js/edn-serde))))
  builder)

(defn transformer
  "Takes a transducer and creates an instance of
  org.apache.kafka.streams.kstream.Transformer with overrides for
  init, transform, and close."
  [xf]
  (let [ctx (atom nil)]
    (reify
      Transformer
      (init [_ context]
        (reset! ctx context))
      (transform [_ k v]
        (let [^KeyValueStore store (.getStateStore @ctx "state")]
          (doseq [[result-k result-v] (first (sequence (xf store) [[k v]]))]
            (.forward @ctx result-k result-v))))
      (close [_]))))


(defn transduce
  "Applies the transducer xf to each element of the kstream."
  [kstream xf]
  (j/transform kstream (fn [] (transformer xf)) ["state"]))
#+END_SRC

** Topology

  There is a lot to unpack here but the main takaways here are that there are 2
  main sides to the topology happening here. As you may recall from the diagram
  above on one side we are going to collect and validate data comming in from
  upstream systems.

  The other side of the topology is going to listen for trigger events and post
  to a dummy SBA endpoint.

  Notice the extensive use of logging with our custom logging function that also
  produces metrics for the log events. Naming yout log and metrics events makes
  for easier debugging and tracing later on.

  Also notice that this namespace does not reference any config directly. It all
  is passed in through our Integrant initializer defined at the bottom.

#+BEGIN_SRC clojure :tangle prod-app/src/prod_app/topology.clj :results silent
(ns prod-app.topology
  (:gen-class)
  (:require [clj-http.client :as http]
            [clj-uuid :as uuid]
            [clojure.data.json :as json]
            [clojure.spec.alpha :as s]
            [clojure.walk :as walk]
            [prod-app.log :as log]
            [prod-app.xform :as jxf]
            [prod-app.specs.reader-specs :as r-specs]
            [prod-app.specs.writer-specs :as w-specs]
            [integrant.core :as ig]
            [jackdaw.streams :as j]))

(defn loan-application
  "returns sba loan application from external data"
  [external-loan-application]
  (let [external-opportunity-id (:opportunity-id external-loan-application)]
    (assoc external-loan-application :loan-application-id
           (uuid/v5 uuid/+namespace-url+ external-opportunity-id))))

(defn update-loan-application
  [state & {:keys [swap-fn registry]}]
  (fn [rf]
    (fn
      ([] (rf))
      ([result] (rf result))
      ([result record]
       (let [[_ v] record
             id (uuid/v5 uuid/+namespace-url+ (:opportunity-id v))
             metadata {:id id
                       :published-timestamp (System/currentTimeMillis)
                       :published-by "sba-connector"}
             loan-app (loan-application v)
             opportunity-id (:opportunity-id loan-app)]
         (if (s/valid? ::w-specs/loan-application loan-app)
           (let [next (as-> loan-app %
                        (swap-fn state merge {opportunity-id %})
                        (get % opportunity-id)
                        (do
                          (log/logger
                           {:level :info
                            :event "loan-application-attribute-validation-success"
                            :metrics-registry registry
                            :message
                            "Loan application attributes satisfy writer spec"}
                           v %)
                          %)
                        (if (s/valid? ::r-specs/loan-application %)
                          (do
                            (log/logger
                             {:level :info
                              :event "loan-application-complete"
                              :metrics-registry registry
                              :message
                              "Loan application satisfies reader spec"}
                             v %)
                            (assoc %
                                   :loan-application-is-complete true
                                   :problems []))
                          (let [problems (:clojure.spec.alpha/problems
                                          (s/explain-data ::r-specs/loan-application %))]
                            (log/logger
                             {:level :info
                              :event "loan-application-incomplete"
                              :problems-count (count problems)
                              :metrics-registry registry
                              :message
                              "Loan application does not satisfy reader spec"}
                             v %)
                            (assoc %
                                   :loan-application-is-complete false
                                   :problems (map str problems))))
                        (merge % metadata)
                        (vector opportunity-id %)
                        (vector %))]
             (rf result next))
           (do
             (log/logger
              {:level :info
               :event "loan-application-attribute-validation-failure"
               :metrics-registry registry
               :message
               "Loan application attributes do not satisfy writer spec"}
              v)
             (rf result []))))))))

(defn parse-sba-http-response
  "Parse sba post request. Gracefully handles a non-json response."
  [response]
  (let [response-data (try (-> (:body response)
                               json/read-str)
                           (catch Exception e
                             {"loan-number" false}))
        loan-number (get response-data "loan-number")]
    {:status (if loan-number "success" "failure")
     :loan-number loan-number
     :result (json/write-str response-data)}))

(defn send-loan-application-to-sba
  [state & {:keys [deref-fn get-fn config registry]}]
  (fn [rf]
    (fn
      ([] (rf))
      ([result] (rf result))
      ([result record]
       (let [[_ v] record
             opportunity-id (:opportunity-id v)
             request-body (json/write-str {:dummy-request (str loan-application)})
             loan-application (get-fn (deref-fn state) opportunity-id)
             loan-application (into {} (remove (comp nil? val) loan-application))
             id (uuid/v5 uuid/+namespace-url+ (:trigger-id v))
             metadata {:id id
                       :published-timestamp (System/currentTimeMillis)
                       :published-by "sba-connector"}]

         (cond
           (nil? loan-application)
           (do
             (log/logger
              {:level :warn
               :event "unknown-loan-application"
               :message "Could not find matching loan application for trigger, ignoring"}
              v metadata {:opportunity-id opportunity-id})
             (rf result []))

           (s/valid? ::r-specs/loan-application loan-application)
           (let [url (get-in config [:sba-config :url])
                 body request-body
                 _ (log/logger
                    {:level :info
                     :event "sba-http-request"
                     :message "New HTTP request to SBA"
                     :metrics-registry registry
                     :body body
                     :url url}
                    v loan-application)
                 response (http/post url {:headers {"content-type" "application/json"}
                                          :body body})
                 next (as-> response %
                        (do (log/logger
                             {:level :debug
                              :event "unparsed-sba-response"
                              :body response
                              :metrics-registry registry
                              :message
                              "Unparsed SBA API post response"}
                             v loan-application metadata)
                            %)
                        (merge (parse-sba-http-response %)
                               loan-application
                               metadata)
                        (do (log/logger
                             {:level :info
                              :event "sba-response-result"
                              :metrics-registry registry
                              :message
                              "SBA response result"}
                             v loan-application metadata)
                            %)
                        (vector opportunity-id %)
                        (vector %))]
             (rf result next))

           :else
           (let [_ (as-> {} %
                     (merge  % {:status "cancelled"
                                :loan-number nil
                                :result (str "Could not send HTTP request. "
                                             "The loan application does not satisfy the reader spec.")}
                             loan-application
                             metadata)
                     (do
                       (log/logger
                        {:level :warn
                         :event "request-cancelled-loan-application-incomplete"
                         :metrics-registry registry
                         :message (:sba/result %)}
                        %)
                       %)
                     (vector opportunity-id %)
                     (vector %))]
             (rf result []))))))))

(defn topology-builder
  [{:keys [external-loan-application
           external-trigger
           sba-loan-application-updated
           sba-result-available]}
   xforms
   registry]
  (fn [builder]
    (jxf/add-state-store! builder)
    (-> (j/kstream builder external-loan-application)
        (j/peek (fn [[k v]]
                  (log/logger
                   {:level :info
                    :opportunity-id k
                    :event "new-external-loan-application"
                    :metrics-registry registry
                    :message
                    "New external loan application snapshot"}
                   v external-loan-application)))
        (jxf/transduce (::update-loan-application xforms))
        (j/peek (fn [[k v]]
                  (log/logger
                   {:level :info
                    :opportunity-id k
                    :event "sba-loan-application-updated-event"
                    :metrics-registry registry
                    :message
                    "SBA loan application updated "}
                   v sba-loan-application-updated)))
        (j/to sba-loan-application-updated))

    (-> (j/kstream builder external-trigger)
        (j/peek (fn [[k v]]
                  (log/logger
                   {:level :info
                    :opportunity-id k
                    :event "external-trigger-event"
                    :metrics-registry registry
                    :message
                    "New external trigger"}
                   v external-trigger)))
        (jxf/transduce (::send-loan-application-to-sba xforms))
        (j/peek (fn [[k v]]
                  (log/logger
                   {:level :info
                    :opportunity-id k
                    :event "sba-result-available-event"
                    :metrics-registry registry
                    :message "SBA result available"}
                   v sba-result-available)))
        (j/to sba-result-available))
    builder))

(defmethod ig/init-key ::app [_ {:keys [config topology] :as opts}]
  (let [streams-app (j/kafka-streams topology (:streams-config config))]
    (log/info "Started sba-connector streams app")
    (j/start streams-app)
    (assoc opts :streams-app streams-app)))
#+END_SRC

** Testing the topology

  Lets implement a couple tests. This demo does not coitain full test coverage!
  In this tests we are going to validate one side of the topology. Notice how we
  use an atom to simulate our state store. The test requires very little setup.
  We simply need to ~transduce~ with our transformation fn we defined but that
  part of the topology. Neat!

#+BEGIN_SRC clojure :tangle prod-app/test/prod_app/topology_test.clj :results silent
(ns prod-app.topology-test
  (:require [clojure.test :refer [deftest is testing]]
            [clojure.edn :as edn]
            [clojure.spec.alpha :as s]
            [clojure.spec.gen.alpha :as gen]
            [prod-app.topology :as sc]
            [prod-app.log :as log]
            [prod-app.specs.reader-specs :as r-specs]
            [prod-app.specs.writer-specs :as w-specs]
            [metrics.core :as metrics]
            [metrics.meters :as meters]))

(defn gen-external-loan-app []
  (gen/generate (s/gen ::r-specs/loan-application)))

(defn metric-total [registry metric-name]
  (:total (meters/rates
           (meters/meter
            registry
            (log/->metric-name metric-name)))))

(deftest update-loan-application-test
  (testing "valid loan app"
    (let [state (atom {}) ;; yay transducers !!
          registry (metrics/new-registry)
          external-loan-application (gen-external-loan-app)
          opportunity-id (:opportunity-id external-loan-application)
          [[k v]] (transduce
                   (sc/update-loan-application state
                                               :swap-fn swap!
                                               :registry registry)
                   concat
                   [[opportunity-id external-loan-application]])]
      (is (= opportunity-id k) "output record key matches the opportunity-id")
      (is (s/valid? ::w-specs/sba-loan-application-updated-event v))
      (is (= opportunity-id (:opportunity-id v))
          "input opportunity-id matches the output opportunity-id")
      (is (= true (:loan-application-is-complete v))
          "loan application is set to complete")
      (is (nil? (not-empty (:problems v)))
          "problems are empty")
      (is (= 1 (metric-total registry "loan-application-complete")))))

  (testing "invalid loan app"
    (let [state (atom {})
          registry (metrics/new-registry)
          external-loan-application (dissoc (gen-external-loan-app)
                                            :tax-id)
          opportunity-id (:opportunity-id external-loan-application)
          [[_ v]] (transduce
                   (sc/update-loan-application state
                                               :swap-fn swap!
                                               :registry registry)
                   concat
                   [[opportunity-id external-loan-application]])]
      (is (= false (:loan-application-is-complete v))
          "loan application is set to incomplete")
      (is (not-empty (:problems v))
          "includes the problems")
      (is (= 1 (metric-total registry "loan-application-incomplete"))))))
#+END_SRC

  Ok lets run out tests.

#+begin_src clojure :ns clojure.test :tangle prod-app/dev/scratch.clj :results output :exports both
(run-tests 'prod-app.topology-test)
#+end_src

#+RESULTS:
: 
: Testing prod-app.topology-test
: 
: Ran 1 tests containing 9 assertions.
: 0 failures, 0 errors.


  We get nice, readable logs with our log configuration for tests. Notice the
  ~:wrap~ header argument on the following snippet.

#+BEGIN_SRC bash :dir prod-app :results output :wrap EXPORT json :exports both
tail -n 31 log/sba-connector-test.log 
#+END_SRC

#+RESULTS:
#+begin_EXPORT json
  "line" : 53,
  "opportunity-id" : "Y8vcTAKn2Hx5K3",
  "published-timestamp" : 1592784425178,
  "column" : 17,
  "requested-amount" : "1",
  "id" : "d5d779e2-8edf-584b-8243-1a587221b902",
  "event" : "sba-response-result",
  "tax-id" : "6126019115",
  "logger" : "prod-app.log",
  "message" : "SBA response result",
  "context" : "default"
}
{
  "timestamp" : "2020-06-22T00:07:05.925Z",
  "level" : "INFO",
  "thread" : "prod-app-StreamThread-5",
  "loan-application-id" : "548a46b0-fa61-58e8-9dbf-52f60e3c0280",
  "published-by" : "sba-connector",
  "ns" : "prod-app.log",
  "line" : 53,
  "opportunity-id" : "Y8vcTAKn2Hx5K3",
  "published-timestamp" : 1592784425178,
  "column" : 17,
  "requested-amount" : "1",
  "id" : "d5d779e2-8edf-584b-8243-1a587221b902",
  "event" : "sba-result-available-event",
  "tax-id" : "6126019115",
  "logger" : "prod-app.log",
  "message" : "SBA result available",
  "context" : "default"
}
#+end_EXPORT

** The rest of the structure

  Before we move to doing integration testing we have to build the rest of the
  structure for a Jackdaw app to run. for this we rely heavily on Integrant.

  We describe each of the structural parts of the system. We start with
  exception handling. Nothing exceptinal here :D We setup our uncaught exception
  handler to report to honeybadger.

*** Exception handling

#+BEGIN_SRC clojure :tangle prod-app/src/prod_app/exception.clj :results silent
(ns prod-app.exception
  (:require [prod-app.log :as log]
            [honeybadger.core :as honeybadger]
            [integrant.core :as ig]))

(def magic-keys
  "The keys that honeybadger treats special in its metadata."
  [:tags :component :action :context :request])

(defn with-app-meta
  [app raw-metadata]
  (assoc-in raw-metadata [:context :app] app))

(defn groom-meta
  "Cleans up the metadata for honeybadger so we see the data
  we expect in the places we expect.
  Pulls out the magic keys, merges the rest under :context where anything
  goes."
  [raw-metadata]
  (let [;; all the special keys are in this map
        predefined (select-keys raw-metadata magic-keys)
        ;; all the non-magic keys are in this map
        added-context (apply dissoc raw-metadata magic-keys)]
    (update predefined :context merge added-context)))

(defn hb-notify
  "Notifies Honeybadger of the error.
  `error` can be a string or exception object.
  `metadata` has a specific set of keys supported by honeybadger, others are ignored,
  see the select-keys call, and https://github.com/camdez/honeybadger#metadata"
  [config error raw-metadata]
  (let [metadata (->> raw-metadata
                      (groom-meta)
                      (with-app-meta (:app config)))]
    (log/error {:error error
                :metadata raw-metadata}
               "Notifying HoneyBadger")
    @(honeybadger/notify config error metadata)))

(defn terminate
  "Stop the JVM and exit with an error code."
  []
  (shutdown-agents) ; this may be a no-op
  (System/exit 1))

(defmethod ig/init-key ::honeybadger [_ {:keys [config]}]
  (let [hb-report (partial hb-notify (:honeybadger config))
        handler (reify Thread$UncaughtExceptionHandler
                  (uncaughtException [this thread error]
                    (try
                      (hb-report error {})
                      (catch Throwable t
                        (log/error {:uncaught-exception error
                                    :uncaught-exception-handler-error t}
                                   "UncaughtExceptionHandler fn threw Exception"))
                      (finally (terminate)))))]
    ;; set the handler if no other code catches an error
    (Thread/setDefaultUncaughtExceptionHandler handler)
    ;; return the reporting function and the exception handler so they can be used
    ;; in other contexts (eg. kafka streams can use an uncaught exception handler as well)
    {:report hb-report
     :handler handler}))
#+END_SRC

*** Metrircs
    
  At FC we use prometheus for log collection.

#+BEGIN_SRC clojure :tangle prod-app/src/prod_app/metrics.clj :results silent
(ns prod-app.metrics
  (:require [prod-app.log :as log]
            [integrant.core :as ig]
            [metrics.core :refer [new-registry]]
            [metrics.reporters.jmx :as jmx]))

(defmethod ig/init-key ::registry
  [_ _]
  (log/info "Created metrics registry")
  (new-registry))

(defmethod ig/init-key ::prometheus-reporter
  [_ {:keys [registry]}]
  (if registry
    (let [reporter (jmx/reporter registry {:domain "fundingcircle"})]
      (jmx/start reporter)
      (log/info "Initialised Prometheus metrics reporter")
      reporter)
    {:enabled false}))
#+END_SRC

*** Streams

  These are our topology building and running facilities.

#+BEGIN_SRC clojure :tangle prod-app/src/prod_app/streams.clj :results silent
(ns prod-app.streams
  (:require [integrant.core :as ig]
            [jackdaw.streams :as j]))

;; factored out from init-key method for use in integration tests
(defn build-topology
  [config topology-builder topic-metadata xforms deref-fn get-fn swap-fn registry]
  (let [xform-map (into {}
                        (map (fn [f]
                               (let [k (keyword (str (:ns (meta f)))
                                                (str (:name (meta f))))
                                     v #(f %
                                           :config config
                                           :deref-fn deref-fn
                                           :get-fn get-fn
                                           :swap-fn swap-fn
                                           :registry registry)]
                                 [k v]))
                             xforms))]
    (topology-builder topic-metadata xform-map registry)))

(defmethod ig/init-key ::topology [_ {:keys [config
                                             topology-builder
                                             topics
                                             xforms
                                             deref-fn
                                             get-fn
                                             swap-fn
                                             registry]}]
  (let [build-fn (build-topology config
                                 topology-builder
                                 topics
                                 xforms
                                 deref-fn
                                 get-fn
                                 swap-fn
                                 registry)
        streams-builder (j/streams-builder)]
    (build-fn streams-builder)))
#+END_SRC

*** Topic Metadata
 
  Topic metadata is our topic configuration. This is used to setup a topic
  serializer and desirializers. It is also where we track other Kafka topic
  configurations like replication factor and partition count. I've manually
  added the AVRO schemas to the ~schemas~ folder.

#+BEGIN_SRC clojure :tangle prod-app/src/prod_app/topic_metadata.clj :results silent
(ns prod-app.topic-metadata
  (:require [clojure.edn :as edn]
            [integrant.core :as ig]))

(defn build-topic-metadata
  [replication-factor partition-count]
  {:external-loan-application
   {:topic-name "external-loan-application-1"
    :partition-count partition-count
    :replication-factor replication-factor
    :key-serde {:serde-keyword :jackdaw.serdes/string-serde}
    :value-serde {:serde-keyword :jackdaw.serdes.avro.confluent/serde
                  :schema-filename "schemas/external-loan-application.json"
                  :key? false}}

   :external-trigger
   {:topic-name "external-trigger-1"
    :partition-count partition-count
    :replication-factor replication-factor
    :key-serde {:serde-keyword :jackdaw.serdes/string-serde}
    :value-serde {:serde-keyword :jackdaw.serdes.avro.confluent/serde
                  :schema-filename "schemas/external-trigger.json"
                  :key? false}}

   :sba-loan-application-updated
   {:topic-name "sba-loan-application-updated-1"
    :register-schema? true
    :partition-count partition-count
    :replication-factor replication-factor
    :key-serde {:serde-keyword :jackdaw.serdes/string-serde}
    :value-serde {:serde-keyword :jackdaw.serdes.avro.confluent/serde
                  :schema-filename "schemas/sba-loan-application-updated.json"
                  :key? false}}

   :sba-result-available
   {:topic-name "sba-result-available-1"
    :register-schema? true
    :partition-count partition-count
    :replication-factor replication-factor
    :key-serde {:serde-keyword :jackdaw.serdes/string-serde}
    :value-serde {:serde-keyword :jackdaw.serdes.avro.confluent/serde
                  :schema-filename "schemas/sba-result-available.json"
                  :key? false}}})


(defmethod ig/init-key ::sba-connector [_ {:keys [config]}]
  (let [replication-factor (edn/read-string (:replication-factor config))]
    (build-topic-metadata replication-factor 100)))
#+END_SRC

*** Topics

  This is where topic metadata gets initialized.

#+BEGIN_SRC clojure :tangle prod-app/src/prod_app/topics.clj :results silent
(ns prod-app.topics
  (:require [clojure.algo.generic.functor :as functor]
            [clj-http.client :as client]
            [prod-app.log :as log]
            [clojure.data.json :as json]
            [clojure.java.io :as io]
            [integrant.core :as ig]
            [jackdaw.admin :as ja]
            [jackdaw.serdes.avro :as jsa]
            [jackdaw.serdes.avro.schema-registry :as sr]
            [jackdaw.serdes.resolver :as resolver]))

(defn slurp-avro
  "Slurps a serde."
  [filename]
  (if-let [resource (io/resource filename)]
    (slurp resource)
    (throw (ex-info
            (format "Didn't find schema file %s in resources" filename)
            {}))))

(defn register-schema [topic-name filename schema-registry-url]
  (let [schema (slurp-avro filename)
        json-schema-str (-> schema
                            json/read-str
                            json/write-str)
        payload (json/write-str {:schema json-schema-str})
        url (str schema-registry-url
                 "/subjects/" topic-name "-value/versions")
        response (client/post url {:body payload :content-type "application/json"})
        body (-> response :body json/read-str)]
    (when (= (:status response) 200)
      (log/info
       (format "Successfully registered %s schema with id %s"
               topic-name (get body "id"))))))

(def +type-registry-with-uuid-type+
  (merge jsa/+base-schema-type-registry+ jsa/+UUID-type-registry+))

(defn resolver [schema-registry-url]
 (if schema-registry-url
   (resolver/serde-resolver :schema-registry-url schema-registry-url
                            :type-registry +type-registry-with-uuid-type+)
   (resolver/serde-resolver :schema-registry-url ""
                            :type-registry +type-registry-with-uuid-type+
                            :schema-registry-client (sr/mock-client))))

(defn resolve-serdes [topic-metadata schema-registry-url]
  (functor/fmap #(assoc %
                :key-serde ((resolver schema-registry-url) (:key-serde %))
                :value-serde ((resolver schema-registry-url) (:value-serde %)))
        topic-metadata))

(defmethod ig/init-key ::topics [_ {:keys [config topic-metadata]}]
  (log/info "Creating topics if they dont exist")
  (with-open [client (ja/->AdminClient (:client-config config))]
    (try
      (ja/create-topics! client (vals topic-metadata))
      (catch Exception e
        (log/info (str "Couldnt create topic: " (.getMessage e))))))

  (log/info "Registering schemas")
  (doseq [[_ topic-config] (->> topic-metadata
                                (filter #(:register-schema? (second %))))]
    (register-schema (:topic-name topic-config)
                     (get-in topic-config [:value-serde :schema-filename])
                     (:schema-registry-url config)))

  (log/info topic-metadata "Resolving topic metadata")
  (resolve-serdes topic-metadata (:schema-registry-url config)))
#+END_SRC

*** Config

  We finish with the config namespace that will hold the definition of our system.
  The neat thing abut this is that the config namespace becomes the glue that
  connects your system and not a random assortment of helpers.

#+BEGIN_SRC clojure :tangle prod-app/src/prod_app/config.clj :results silent
(ns prod-app.config
  (:require [clojure.java.io :as io]
            [clojure.walk :as walk]
            [prod-app.exception :as exception]
            [prod-app.metrics :as metrics]
            [prod-app.topology :as sba-connector]
            [prod-app.streams :as streams]
            [prod-app.topic-metadata :as topic-metadata]
            [prod-app.topics :as topics]
            [prod-app.xform :as jxf]
            [integrant.core :as ig]
            [outpace.config :as outpace]
            [outpace.config.bootstrap :as config-bootstrap]))

(defmacro defconfig-warn
  "Ensures that any attempt to use outpace/defconfig explicitly errors, rather
  than just acting strangely.
  Outpace offers no method of reloading config without reloading source,
  as it encourages using defconfig to bind configs to the top level of namespaces.
  Our usage of outpace and reloadable config currently requires not using
  defconfig."
  [lookup]
  (throw (ex-info "invalid usage of config for this app, use outpace.config/lookup inside an integrant method instead"
                  {:config-name lookup})))

(alter-var-root #'outpace/defconfig (constantly @#'defconfig-warn))

(defn reload-config
  "Goes into outpace internals to get the config reload semantics we want."
  [source]
  (alter-var-root #'config-bootstrap/explicit-config-source
                  (constantly source))
  (alter-var-root #'outpace/config
                  (constantly (delay (outpace/load-config)))))

(defn interpolate
  "Given a config template fills in environment specific data."
  [template]
  (walk/postwalk (fn [x]
                   (if (symbol? x)
                     (outpace/lookup x)
                     x))
                 template))

(defn get-config-resource
  [resource-name]
  #(io/resource resource-name))

(def streams-config
  '{"application.id" topology/application-id
    "client.id" "prod-app"
    "processing.guarantee" "exactly_once"
    "acks" "all"
    "bootstrap.servers" kafka/bootstrap-servers
    "replication.factor" kafka/replication-factor
    "cache.max.bytes.buffering" "0"
    "num.stream.threads" "5"})

(def client-config
  '{"bootstrap.servers" kafka/bootstrap-servers})

(def honeybadger
  '{:api-key honeybadger/key
    :env honeybadger/env
    :app :sba-connector})

(def sba-config
  '{:url sba/url})

(def config-template
  "A template that is filled in via outpace (see common.config)
  Each submap should apply to a specific domain of interest and mix symbols,
  which will be looked up in the config map loaded by outpace, with configs
  that don't change on a per-environment basis."
  {:streams-config streams-config
   :client-config client-config
   :sba-config sba-config
   :schema-registry-url 'kafka/schema-registry-url
   :replication-factor 'kafka/replication-factor})

(def defaults
  "Default config loading data.
  This is pulled out of the init-key (and comes in via the core ns) in order to
  simplify using alternate configs without restarting the repl."
  {:get-source (get-config-resource "config.edn")
   :template config-template})

(defmethod ig/init-key ::config [_ {:keys [get-source template]}]
  (reload-config (get-source))
  (let [unprepared-config (interpolate template)]
    ;; apply transformations to config based on flags
    unprepared-config))

(def sba-connector-app-state
  "App state for sba connector streams app."
  {::config defaults
   ::topic-metadata/sba-connector {:config (ig/ref ::config)}
   ::topics/topics {:config (ig/ref ::config)
                    :topic-metadata (ig/ref ::topic-metadata/sba-connector)}
   ;; Provides functions for error handling
   ::exception/honeybadger {:config (ig/ref ::config)}
   ;; initialize the metrics registry
   ::metrics/registry {}
   ;; initialize the metrics reporter
   ::metrics/prometheus-reporter {:registry (ig/ref ::metrics/registry)}
   ;; Provides a kafka builder topology
   ::streams/topology {:config (ig/ref ::config)
                       :topology-builder sba-connector/topology-builder
                       :topics (ig/ref ::topics/topics)
                       :xforms [#'sba-connector/update-loan-application
                                #'sba-connector/send-loan-application-to-sba]
                       :deref-fn identity
                       :get-fn jxf/kv-store-get-fn
                       :swap-fn jxf/kv-store-swap-fn
                       :registry (ig/ref ::metrics/registry)}
   ;; Provides a started kafka streams
   ::sba-connector/app {:config (ig/ref ::config)
                        :topics (ig/ref ::topics/topics)
                        :topology (ig/ref ::streams/topology)}})
#+END_SRC

  We define the corresponding ~config.edn~ file

#+begin_src clojure :tangle prod-app/resources/config.edn :eval no
{honeybadger/env #config/or [#config/env "HONEYBADGER_ENVIRONMENT" "test"]
 honeybadger/key #config/env "HONEYBADGER_API_KEY"
 kafka/bootstrap-servers #config/or [#config/env "BOOTSTRAP_SERVERS"
                                     "localhost:9092"]
 kafka/schema-registry-url #config/or [#config/env "SCHEMA_REGISTRY_URL"
                                       "http://localhost:8081"]
 kafka/replication-factor #config/or [#config/env "REPLICATION_FACTOR" "1"]
 topology/application-id #config/or [#config/env "APPLICATION_ID" "sba-connector-streams"]
 sba/url #config/or [#config/env 
                     "SBA_URL" 
                     "https://run.mocky.io/v3/6ccb9c3d-2521-4b2b-b170-6f1d4d022526"]}
#+end_src

** Integration tests

  Ok its time to showcase Test Machine. Test Machine is agnostic to the system
  is testing so it requires quite a bit of scaffolding to make it look nice
  running in your typical test setup. I think the name can be deceiving as Test
  Machine can be used in many contexts outside of the context of regular tests.
  It can also be used to programatically seed environments for example. I think
  some version of this scaffolding will be part of Jackdaw at some point. I've
  added this scaffolding namespaces directly to the generated project to avoid
  noise in this document but be sure to go check them out later.

#+begin_src clojure :tangle prod-app/dev/scratch.clj
(require '[prod-app.integration.fixtures]
         '[prod-app.integration.journal]
         '[prod-app.integration.runner])
#+end_src

#+RESULTS:
: nil

  Test Machine simulates Kafka via commands. The simplest way that I can explain
  it is as a machine that takes input commands and a topoloy configuration and
  spits out on the otherside a detailed log of what what happened on the other
  side of running all the commands given.

#+BEGIN_SRC clojure :tangle prod-app/test/prod_app/integration/topology_test.clj :results silent
(ns prod-app.integration.topology-test
  (:require
   [clojure.test :as t :refer [deftest is testing]]
   [clojure.spec.alpha :as s]
   [clojure.spec.gen.alpha :as gen]
   [jackdaw.test :as jd.test]
   [jackdaw.test.commands.watch :as watch]
   [metrics.core :as metrics]
   [prod-app.integration.runner :as test-runner]
   [prod-app.integration.journal :as journal]
   [prod-app.specs.reader-specs :as r-specs]
   [prod-app.specs.writer-specs :as w-specs]
   [prod-app.config :as config]
   [prod-app.specs.attributes]
   [prod-app.streams :as streams]
   [prod-app.topics :as topics]
   [prod-app.topic-metadata :as topic-metadata]))

(defn topology-under-test [tmd]
  (let [{:keys [config topology-builder topic-metadata xforms deref-fn get-fn swap-fn]}
        (assoc (::streams/topology config/sba-connector-app-state)
               :topic-metadata tmd)]
    (streams/build-topology config
                            topology-builder
                            topic-metadata
                            xforms
                            deref-fn
                            get-fn
                            swap-fn
                            (metrics/new-registry))))

(defn run-integration-test [mode commands assertions]
  (binding [watch/*default-watch-timeout* (if (= :mock mode) 1000 10000)]
    (let [tmd (topics/resolve-serdes (topic-metadata/build-topic-metadata 1 1) false)]
      (test-runner/run-test
       mode tmd (topology-under-test tmd)
       (fn [machine]
         ;; Run the test
         (let [{:keys [results journal]} (jd.test/run-test machine commands)]
           (is (test-runner/result-ok? results))
           (journal/summarise-and-export journal)
           (assertions journal)))))))

(defn mock-sba-endpoint [mock-responder]
  (fn [url post-body]
    (let [{:keys [headers body]} post-body]
      (is (= "application/json" (get headers "content-type")))
      (mock-responder body))))

(defmacro with-mock-sba-endpoint [[mock-responder] & body]
  `(with-redefs [clj-http.client/post (mock-sba-endpoint ~mock-responder)]
     ~@body))

(defn gen-external-loan-app []
  (gen/generate (s/gen ::r-specs/loan-application)))

(deftest integration-test
  (testing "SBA Builder"
    (doseq [[api-response loan-application-fn] [[{:status 200
                                                  :headers {"server" "da-government-box"}
                                                  :body
                                                  "{\"loan-number\": \"123\"}"}
                                                 gen-external-loan-app]]]
      (with-mock-sba-endpoint [(fn [req]
                                 api-response)]
        (let [loan-application (loan-application-fn)
              opportunity-id (:opportunity-id loan-application)]
          (run-integration-test
           :mock
           [[:write! :external-loan-application loan-application {:key opportunity-id}]
            [:watch (fn [j]
                      (let [ms (journal/messages j :sba-loan-application-updated)]
                        (> (count ms) 0)))]
            [:write!
             :external-trigger
             {:opportunity-id opportunity-id
              :trigger-id (java.util.UUID/randomUUID)}
             {:key opportunity-id}]
            [:watch (fn [j]
                      (let [ms (journal/messages j :sba-result-available)]
                        (> (count ms) 0)))]]
           (fn [j]
             (let [ms (journal/messages j :sba-result-available)
                   result (first ms)]
               (is (s/valid? ::w-specs/sba-result-available-event result))
               (is (= "123" (:loan-number result)))
               (is (= opportunity-id (:opportunity-id result)))))))))))

#+END_SRC 

#+begin_src clojure :ns clojure.test :tangle prod-app/dev/scratch.clj :results output :exports both
(run-tests 'prod-app.integration.topology-test)
#+end_src

#+RESULTS:
#+begin_example

Testing prod-app.integration.topology-test

|                        :topic | :messages |
|-------------------------------+-----------|
| :sba-loan-application-updated |         1 |
|         :sba-result-available |         1 |
writing results to './test-results/journal-1592784859616'

Ran 1 tests containing 5 assertions.
0 failures, 0 errors.
#+end_example

** Running our app

  Now lets see how to run this locally to showcase how this app would actually
  run in prod with a running Kafka. First we need an entrypoint.

#+begin_src clojure :tangle prod-app/src/prod_app/core.clj :results silent
(ns prod-app.core
  (:require [prod-app.config :as config]
            [integrant.core :as ig]))

;;  This hash-map will get filled in with data carrying stateful
;;  resources defined via integrant.
;;  After starting the services you can find the resources they
;;  provide under their defining keys.
(defonce app
  {})

(defn fresh-app
  [state]
  (def app (ig/init state)))

;; TODO: figure out a better way to do the main thats safe
(defn -main
  "Reloads config.
  Starts and binds the running app."
  []
  (fresh-app config/sba-connector-app-state))
#+end_src
  
  We need a runtime environment to run our example. For that we'll rely on
  docker-compose.

#+begin_src yaml :tangle prod-app/docker-compose.yml :eval no
---
version: '2'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:5.5.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-kafka:5.5.0
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper
    ports:
      - "29092:29092"
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0

  schema-registry:
    image: confluentinc/cp-schema-registry:5.5.0
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - zookeeper
      - broker
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'
#+end_src

  Now we can define a "slow" and manual end to end test for our topology running
  on Kafka environment. This also helps us understand what kind of setup our app
  woulr need in production. Be patient :P

#+BEGIN_SRC bash :dir prod-app :tangle prod-app/bin/end-to-end.bash :results pp :exports both
#!/usr/bin/bash
echo "clear the logs"
rm log/sba-connector-test.log

echo "resetting the runtime"
docker-compose down -v
docker-compose up -d schema-registry

echo "waiting for schema registry up"
sleep 10

echo "running our topology"
clojure -C:test -m prod-app.core &
echo "registering external schemas"
echo '{"schema": ""}' | jq --rawfile foo resources/schemas/external-trigger.json '. |= {schema: $foo, schemaType: "AVRO"}' | curl -X POST -H "Content-Type: application/json" http://localhost:8081/subjects/external-trigger-1-value/versions -d @-
echo '{"schema": ""}' | jq --rawfile foo resources/schemas/external-loan-application.json '. |= {schema: $foo, schemaType: "AVRO"}' | curl -X POST -H "Content-Type: application/json" http://localhost:8081/subjects/external-loan-application-1-value/versions -d @-


echo "producing external loan application"
{
  eval docker-compose exec -T schema-registry kafka-avro-console-producer \
       --bootstrap-server broker:29092 \
       --topic external-loan-application-1 \
       --property value.schema=\'$(cat resources/schemas/external-loan-application.json)\' \
       --property parse.key=true \
       --property key.schema=\'$(echo '{"type":"string"}')\' \
       --property key.separator=\" \" \
       <<< '"Y8vcTAKn2Hx5K3" {"opportunity_id": {"string": "Y8vcTAKn2Hx5K3"}, "requested_amount": {"string": "1"}, "loan_application_id": {"string": "0b62206c-d244-4a4f-80ce-daa164934b53"}, "tax_id": {"string": "6126019115"}}' 
} &> /dev/null # supresess the super verbose utility

echo "producing a trigger"
{
  eval docker-compose exec -T schema-registry kafka-avro-console-producer \
    --bootstrap-server broker:29092 \
    --topic external-trigger-1 \
    --property value.schema=\'$(cat resources/schemas/external-trigger.json)\' \
    --property parse.key=true \
    --property key.schema=\'$(echo '{"type":"string"}')\' \
    --property key.separator=\" \" \
    <<< '"Y8vcTAKn2Hx5K3" {"opportunity_id": {"string": "Y8vcTAKn2Hx5K3"}, "trigger_id": {"string": "7d7c6ca4-71e0-4228-a231-6a5449a864e7"}}' 
} &> /dev/null # surpreses the super verbose utility
echo "Wait for records to be processed"
sleep 10
pkill -P $$
echo "Done"
#+END_SRC

#+RESULTS:
: clear the logs
: resetting the runtime
: waiting for schema registry up
: running our topology
: registering external schemas
: {"id":1}{"id":2}producing external loan application
: producing a trigger
: Wait for records to be processed
: Done

  After the run we can see the topics that were created by our topology

#+BEGIN_SRC bash :dir prod-app
docker-compose exec -T broker kafka-topics --list --bootstrap-server localhost:9092
#+END_SRC

#+RESULTS:
| __confluent.support.metrics           |
| __consumer_offsets                    |
| __transaction_state                   |
| _schemas                              |
| external-loan-application-1           |
| external-trigger-1                    |
| sba-connector-streams-state-changelog |
| sba-loan-application-updated-1        |
| sba-result-available-1                |

  The internal schemas where defined by the topology when it was writing to
  them. The external ones we had to create manually in the script above.

#+BEGIN_SRC bash :dir prod-app
curl http://localhost:8081/subjects | jq .
#+END_SRC

#+RESULTS:
| [                                    |
| external-loan-application-1-value    |
| sba-loan-application-updated-1-value |
| external-trigger-1-key               |
| external-trigger-1-value             |
| external-loan-application-1-key      |
| sba-result-available-1-value         |
| ]                                    |

  We can peek at our logs and we see that a sba result was published.

#+BEGIN_SRC bash :dir prod-app :results output :wrap EXPORT json :exports both
tail -n 19 log/sba-connector-test.log
#+END_SRC

#+RESULTS:
#+begin_EXPORT json
{
  "timestamp" : "2020-06-22T00:14:47.412Z",
  "level" : "INFO",
  "thread" : "prod-app-StreamThread-5",
  "loan-application-id" : "548a46b0-fa61-58e8-9dbf-52f60e3c0280",
  "published-by" : "sba-connector",
  "ns" : "prod-app.log",
  "line" : 53,
  "opportunity-id" : "Y8vcTAKn2Hx5K3",
  "published-timestamp" : 1592784886563,
  "column" : 17,
  "requested-amount" : "1",
  "id" : "d5d779e2-8edf-584b-8243-1a587221b902",
  "event" : "sba-result-available-event",
  "tax-id" : "6126019115",
  "logger" : "prod-app.log",
  "message" : "SBA result available",
  "context" : "default"
}
#+end_EXPORT

  We can also validate the content of our topics. Use ~kafkacat~ for consuming
  from kafka topic. Its way better than the ~kafka-console-consumor~ and it
  recently added support for AVRO

 #+BEGIN_SRC bash :dir prod-app :results output :wrap EXPORT json :exports both
docker run --tty \
  --network host \
  edenhill/kafkacat:1.5.0 \
  kafkacat -e -q  -b localhost:9092 \
  -s avro -r http://localhost:8081 \
  -t sba-loan-application-updated-1 | jq .
#+END_SRC

#+RESULTS:
#+begin_EXPORT json
{
  "loan_application_id": {
    "string": "548a46b0-fa61-58e8-9dbf-52f60e3c0280"
  },
  "opportunity_id": {
    "string": "Y8vcTAKn2Hx5K3"
  },
  "requested_amount": {
    "string": "1"
  },
  "tax_id": {
    "string": "6126019115"
  },
  "id": {
    "string": "548a46b0-fa61-58e8-9dbf-52f60e3c0280"
  },
  "published_timestamp": {
    "long": 1592784886362
  },
  "published_by": {
    "string": "sba-connector"
  },
  "loan_application_is_complete": {
    "boolean": true
  },
  "problems": {
    "array": []
  }
}
#+end_EXPORT

#+BEGIN_SRC bash :dir prod-app :results output :wrap EXPORT json :exports both
docker run --tty \
  --network host \
  edenhill/kafkacat:1.5.0 \
  kafkacat -e -q  -b localhost:9092 \
  -s avro -r http://localhost:8081 \
  -t sba-result-available-1 | jq .
#+END_SRC

#+RESULTS:
#+begin_EXPORT json
{
  "id": {
    "string": "d5d779e2-8edf-584b-8243-1a587221b902"
  },
  "status": {
    "string": "success"
  },
  "result": {
    "string": "{\"loan-number\":\"123\"}"
  },
  "loan_number": {
    "string": "123"
  },
  "loan_application_id": {
    "string": "548a46b0-fa61-58e8-9dbf-52f60e3c0280"
  },
  "requested_amount": {
    "string": "1"
  },
  "tax_id": {
    "string": "6126019115"
  },
  "published_timestamp": {
    "long": 1592784886563
  },
  "published_by": {
    "string": "sba-connector"
  },
  "opportunity_id": {
    "string": "Y8vcTAKn2Hx5K3"
  }
}
#+end_EXPORT

  We can even inspect our state store backing topic created by Kafka Streams.
  Notice that the data in the backing state store is plain EDN :D

#+BEGIN_SRC bash :dir prod-app :results output :wrap EXPORT clojure :exports both
docker run --tty \
  --network host \
  edenhill/kafkacat:1.5.0 \
  kafkacat -e -q -b localhost:9092  \
  -t sba-connector-streams-state-changelog
#+END_SRC

#+RESULTS:
#+begin_EXPORT clojure
{:loan-application-id #uuid "548a46b0-fa61-58e8-9dbf-52f60e3c0280", :opportunity-id "Y8vcTAKn2Hx5K3", :requested-amount "1", :tax-id "6126019115"}
#+end_EXPORT
